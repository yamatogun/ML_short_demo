{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import inv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I) Example of \"supervised discriminative model\": the Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "datadir = '/home/mastermind/Documents/machine_learning_tuto/data'\n",
    "train_name = 'classificationA.train.txt'\n",
    "test_name = 'classificationA.test.txt'\n",
    "traindata = osp.join(datadir, train_name) \n",
    "testdata = osp.join(datadir, test_name)\n",
    "Xy_train = np.loadtxt(traindata) \n",
    "Xy_test = np.loadtxt(testdata)\n",
    "X_train = Xy_train[:, :2] \n",
    "y_train = Xy_train[:, 2]\n",
    "X_test = Xy_test[:, :2]\n",
    "y_test = Xy_test[:, 2]\n",
    "del Xy_train\n",
    "del Xy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model:\n",
    "\n",
    "\n",
    "$\\mathbb{P}(Y=1|X=x) = \\sigma(w^T x)$\n",
    "\n",
    "$Y|X=x \\sim \\text{Ber}(\\sigma(w^T x))$\n",
    "\n",
    "In particular, linear dependance of $Y$ with relation to $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning : \n",
    "\n",
    "Finding optimal $w$:\n",
    "\n",
    "Training dataset $(x_i, y_i)_{1<=i<=N}$ (sample), assumed IID\n",
    "$y_i \\in \\{0, 1\\}$, $x \\in \\mathbb{R}^p$\n",
    "\n",
    "\n",
    "Several ways to see the optimal value for the weights $w^*$:\n",
    "- Find the maximum likelihood estimator\n",
    "- Minimize a loss criteria (fit the model to the data)\n",
    "\n",
    "We will see that both approaches result to the same quantity to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loglikelihood maximization:\n",
    "\n",
    "\n",
    "Idea: find the model (find $w$) such that our data are quite likely ones / quite realistic / likely to be generated by our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l(w)= \\log \\prod_i p(y_i | x_i, w)$\n",
    "\n",
    "$   = \\sum_i y_i \\log {\\eta_i} + (1-y_i) \\log (1-y_i) $\n",
    "Called cross-entropy (negative).\n",
    "\n",
    "With $\\eta_i=\\sigma(w^T x)$\n",
    "\n",
    "Differentiable and concave. But no closed form for $w^*$. Need an optimization procedure, that is an algorithm able to find an optimal value (here we want to find $w^* = \\underset{w}{\\arg\\max}\\  l(w)$\n",
    "\n",
    "Algorithm we are gonna use: Newton descent, because it is fast and there is a nice interpretation (Often called Iterative Reweighted Least Squares): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{(t+1)} = w^{(t)} - \\nabla^2 l(w^{(t)})^{-1} \\nabla l(w^{(t)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations:\n",
    "\n",
    "Gradient: $\\nabla l = \\sum_i (y_i - \\eta_i)x_i = X^{T}(y-\\eta)$\n",
    "\n",
    "Hessienne: $\\nabla^2 l = - \\sum_i \\eta_i (1-\\eta_i) x_i {x_i}^{T} = - X^T S X$,  with $S = \\text{diag}(\\eta_i (1-\\eta_i)_{1:N})$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Finally we obtain:\n",
    "\n",
    "$w^{(t+1)} = (X^T S^{(t)} X)^{-1} X^T S^{(t)}z^{(t)}$ with $z^{(t)}=Xw^{(t)} + (S^{(t)})^{-1}(y-\\eta^{(t)})$\n",
    "\n",
    "Idea: solve a reweighted least squares problem at each iteration = IRLS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(u):\n",
    "    return 1 / (1+np.exp(-u)) # note: logistic function is always positive (> 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loglikelihood(X, y, w):\n",
    "    eta = logistic(X.dot(w))\n",
    "    return np.sum(y*np.log(eta) + (1-y)*np.log(1-eta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fitLogisticRegression(X, y, niter):\n",
    "    \"\"\"\n",
    "    inputs:\n",
    "    X: samples, n x (p+1)\n",
    "    y: labels (target variable), n x 1 \n",
    "    niter: number of iterations\n",
    "    \n",
    "    output:\n",
    "    optimal parameter vector w\n",
    "    \"\"\"\n",
    "    _, dim = X.shape\n",
    "    w = np.zeros((dim, 1)) # initialization, (p+1) x 1\n",
    "    for i in xrange(niter):\n",
    "        mu = np.squeeze(X.dot(w)) # value of w is updated during each iteration\n",
    "        eta = logistic(mu)\n",
    "        s = eta * (1 - eta)\n",
    "        z = mu + (y - eta) / s # because S is a diagonal matrix\n",
    "        S = np.diag(s)\n",
    "        w = inv((X.T).dot(S).dot(X)).dot(X.T).dot(S).dot(z)\n",
    "        print loglikelihood(X, y, w)\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-31.1475712817\n",
      "-15.8857140368\n",
      "-9.24600861912\n",
      "-5.86347628607\n",
      "-3.92705705057\n",
      "-2.7465876459\n",
      "-1.96839618368\n",
      "-1.41110921718\n"
     ]
    }
   ],
   "source": [
    "# first add a column of 1s to take the bias into account\n",
    "nrows_train, _ = X_train.shape\n",
    "nrows_test, _ = X_test.shape\n",
    "X_train = np.concatenate((X_train, np.ones((nrows_train, 1))), axis=1)\n",
    "X_test = np.concatenate((X_test, np.ones((nrows_test, 1))), axis=1)\n",
    "\n",
    "# Learning: fit model on data\n",
    "w_opt = fitLogisticRegression(X_train, y_train, 8)\n",
    "# apply the optimal model on test dataset:\n",
    "predictions = np.where(logistic(X_test.dot(w_opt))>0.5, 1, 0)\n",
    "classification_rate = float(sum(predictions==y_test))/len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9673333333333334"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
