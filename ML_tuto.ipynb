{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os.path as osp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "datadir = '/home/mastermind/Documents/machine_learning_tuto/data'\n",
    "train_name = 'classificationA.train.txt'\n",
    "test_name = 'classificationA.test.txt'\n",
    "traindata = osp.join(datadir, train_name) \n",
    "testdata = osp.join(datadir, test_name)\n",
    "Xy_train = np.loadtxt(traindata) \n",
    "Xy_test = np.loadtxt(testdata)\n",
    "X_train = Xy_train[:, :2] \n",
    "y_train = Xy_train[:, 2]\n",
    "X_test = Xy_test[:, :2]\n",
    "y_test = Xy_test[:, 2]\n",
    "del Xy_train\n",
    "del Xy_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "MODEL:\n",
    "-----\n",
    "\n",
    "$\\mathbb{P}(Y=1|X=x) = \\sigma(w^T x)$\n",
    "\n",
    "$Y|X=x \\sim \\text{Ber}(\\sigma(w^T x))$\n",
    "\n",
    "In particular, linear dependance of $Y$ with relation to $X$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LEARNING : \n",
    "--------\n",
    "\n",
    "Finding optimal $w$:\n",
    "\n",
    "Training dataset $(x_i, y_i)_{1<=i<=N}$ (sample), assumed IID\n",
    "$y_i \\in \\{0, 1\\}$, $x \\in \\mathbb{R}^p$\n",
    "\n",
    "\n",
    "Several ways to see the optimal value for the weights $w^*$:\n",
    "- Find the maximum likelihood estimator\n",
    "- Minimize a loss criteria (fit the model to the data)\n",
    "\n",
    "We will see that both approaches result to the same quantity to minimize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) Loglikelihood maximization:\n",
    "-----------------------------\n",
    "\n",
    "Idea: find the model (find $w$) such that our data are quite likely ones / quite realistic / likely to be generated by our model.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$l(w)= \\log \\prod_i p(y_i | x_i, w)$\n",
    "\n",
    "$   = \\sum_i y_i \\log {\\eta_i} + (1-y_i) \\log (1-y_i) $\n",
    "Called cross-entropy (negative).\n",
    "\n",
    "With $\\eta_i=\\sigma(w^T x)$\n",
    "\n",
    "Differentiable and concave. But no closed form for $w^*$. Need an optimization procedure, that is an algorithm able to find an optimal value (here we want to find $w^* = \\underset{w}{\\arg\\max}\\  l(w)$\n",
    "\n",
    "Algorithm we are gonna use: Newton descent, because it is fast and there is a nice interpretation (Often called Iterative Reweighted Least Squares): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$w^{(t+1)} = w^{(t)} - \\nabla^2 l(w^{(t)}) \\nabla l(w^{(t)})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computations:\n",
    "$\\nabla l =$\n",
    "\n",
    "$\\nabla^2 l = $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
